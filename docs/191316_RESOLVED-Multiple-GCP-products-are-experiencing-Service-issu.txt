Provider : gcp
Title    : RESOLVED: Multiple GCP products are experiencing Service issues
Published: 2025-06-13T23:48:18+00:00
Link     : https://status.cloud.google.com/incidents/ow5i3PPK96RduMcb1SsW
ID       : tag:status.cloud.google.com,2025:feed:ow5i3PPK96RduMcb1SsW.Z1Rd9fBcpcBK2yggo9W8

--- SUMMARY / DESCRIPTION ---
<p> Incident began at <strong>2025-06-12 10:51</strong> and ended at <strong>2025-06-12 18:18</strong> <span>(all times are <strong>US/Pacific</strong>).</span></p><div class="cBIRi14aVDP__status-update-text"><h1>Incident Report</h1>
<h2><strong>Summary</strong></h2>
<p><em>Google Cloud, Google Workspace and Google Security Operations products experienced increased 503 errors in external API requests, impacting customers.</em></p>
<p><em><strong>We deeply apologize for the impact this outage has had.  Google Cloud customers and their users trust their businesses to Google, and we will do better.  We apologize for the impact this has had not only on our customers’ businesses and their users but also on the trust of our systems.  We are committed to making improvements to help avoid outages like this moving forward.</strong></em></p>
<h3><strong>What happened?</strong></h3>
<p>Google and Google Cloud APIs are served through our Google API management and control planes.  Distributed regionally, these management and control planes are responsible for ensuring each API request that comes in is authorized, has the policy and appropriate checks (like quota) to meet their endpoints.  The core binary that is part of this policy check system is known as Service Control.  Service Control is a regional service that has a regional datastore that it reads quota and policy information from. This datastore metadata gets replicated almost instantly globally to manage quota policies for Google Cloud and our customers.</p>
<p>On May 29, 2025, a new feature was added to Service Control for additional quota policy checks. This code change and binary release went through our region by region rollout, but the code path that failed was never exercised during this rollout due to needing a policy change that would trigger the code.  As a safety precaution, this code change came with a red-button to turn off that particular policy serving path.  The issue with this change was that it did not have appropriate error handling nor was it feature flag protected.  Without the appropriate error handling, the null pointer caused the binary to crash. Feature flags are used to gradually enable the feature region by region per project, starting with internal projects, to enable us to catch issues.  If this had been flag protected, the issue would have been caught in staging.</p>
<p>On June 12, 2025 at ~10:45am PDT, a policy change was inserted into the regional Spanner tables that Service Control uses for policies.  Given the global nature of quota management, this metadata was replicated globally within seconds.  This policy data contained unintended blank fields.  Service Control, then regionally exercised quota checks on policies in each regional datastore. This pulled in blank fields for this respective policy change and exercised the code path that hit the null pointer causing the binaries to go into a crash loop.  This occurred globally given each regional deployment.</p>
<p>Within 2 minutes, our Site Reliability Engineering team was triaging the incident.  Within 10 minutes, the root cause was identified and the red-button (to disable the serving path) was being put in place.  The red-button was ready to roll out ~25 minutes from the start of the incident.  Within 40 minutes of the incident, the red-button rollout was completed, and we started seeing recovery across regions, starting with the smaller ones first.</p>
<p>Within some of our larger regions, such as us-central-1, as Service Control tasks restarted, it created a herd effect on the underlying infrastructure it depends on (i.e. that Spanner table), overloading the infrastructure.  Service Control did not have the appropriate randomized exponential backoff implemented to avoid this.  It took up to ~2h 40 mins to fully resolve in us-central-1 as we throttled task creation to minimize the impact on the underlying infrastructure and routed traffic to multi-regional databases to reduce the load.  At that point, Service Control and API serving was fully recovered across all regions.  Corresponding Google and Google Cloud products started recovering with some taking longer depending upon their architecture.</p>
<h3><strong>What is our immediate path forward?</strong></h3>
<p>Immediately upon recovery, we froze all changes to the Service Control stack and manual policy pushes until we can completely remediate the system.</p>
<h3><strong>How did we communicate?</strong></h3>
<p>We posted our first incident report to Cloud Service Health about ~1h after the start of the crashes, due to the Cloud Service Health infrastructure being down due to this outage. For some customers, the monitoring infrastructure they had running on Google Cloud was also failing, leaving them without a signal of the incident or an understanding of the impact to their business and/or infrastructure. We will address this going forward.</p>
<h3><strong>What’s our approach moving forward?</strong></h3>
<p>Beyond freezing the system as mentioned above, we will prioritize and safely complete the following:</p>
<ul>
<li>We will modularize Service Control’s architecture, so the functionality is isolated and fails open.  Thus, if a corresponding check fails, Service Control can still serve API requests.</li>
<li>We will audit all systems that consume globally replicated data.  Regardless of the business need for near instantaneous consistency of the data globally (i.e. quota management settings are global), data replication needs to be propagated incrementally with sufficient time to validate and detect issues.</li>
<li>We will enforce all changes to critical binaries to be feature flag protected and disabled by default.</li>
<li>We will improve our static analysis and testing practices to correctly handle errors and if need be fail open.</li>
<li>We will audit and ensure our systems employ randomized exponential backoff.</li>
<li>We will improve our external communications, both automated and human, so our customers get the information they need asap to react to issues, manage their systems and help their customers.</li>
<li>We'll ensure our monitoring and communication infrastructure remains operational to serve customers even when Google Cloud and our primary monitoring products are down, ensuring business continuity.</li>
</ul>
<hr />
</div><hr /><p>Affected products: API Gateway, Agent Assist, AlloyDB for PostgreSQL, Apigee, Apigee Edge Public Cloud, Apigee Hybrid, Cloud Data Fusion, Cloud Firestore, Cloud Logging, Cloud Memorystore, Cloud Monitoring, Cloud Run, Cloud Security Command Center, Cloud Shell, Cloud Spanner, Cloud Workstations, Contact Center AI Platform, Contact Center Insights, Data Catalog, Database Migration Service, Dataform, Dataplex, Dataproc Metastore, Datastream, Dialogflow CX, Dialogflow ES, Google App Engine, Google BigQuery, Google Cloud Bigtable, Google Cloud Composer, Google Cloud Console, Google Cloud DNS, Google Cloud Dataflow, Google Cloud Dataproc, Google Cloud Pub/Sub, Google Cloud SQL, Google Cloud Storage, Google Compute Engine, Identity Platform, Identity and Access Management, Looker Studio, Managed Service for Apache Kafka, Memorystore for Memcached, Memorystore for Redis, Memorystore for Redis Cluster, Persistent Disk, Personalized Service Health, Pub/Sub Lite, Speech-to-Text, Text-to-Speech, Vertex AI Online Prediction, Vertex AI Search, Vertex Gemini API, Vertex Imagen API, reCAPTCHA Enterprise</p><p>Affected locations: Johannesburg (africa-south1), Multi-region: asia, Taiwan (asia-east1), Hong Kong (asia-east2), Tokyo (asia-northeast1), Osaka (asia-northeast2), Seoul (asia-northeast3), Mumbai (asia-south1), Delhi (asia-south2), Singapore (asia-southeast1), Jakarta (asia-southeast2), Multi-region: asia1, Sydney (australia-southeast1), Melbourne (australia-southeast2), Multi-region: eu, Multi-region: eur3, Multi-region: eur4, Multi-region: eur5, Warsaw (europe-central2), Finland (europe-north1), Stockholm (europe-north2), Madrid (europe-southwest1), Belgium (europe-west1), Berlin (europe-west10), Turin (europe-west12), London (europe-west2), Frankfurt (europe-west3), Netherlands (europe-west4), Zurich (europe-west6), Milan (europe-west8), Paris (europe-west9), Global, Doha (me-central1), Dammam (me-central2), Tel Aviv (me-west1), Multi-region: nam-eur-asia1, Multi-region: nam10, Multi-region: nam11, Multi-region: nam12, Multi-region: nam13, Multi-region: nam3, Multi-region: nam5, Multi-region: nam6, Multi-region: nam7, Multi-region: nam8, Multi-region: nam9, Montréal (northamerica-northeast1), Toronto (northamerica-northeast2), Mexico (northamerica-south1), São Paulo (southamerica-east1), Santiago (southamerica-west1), Multi-region: us, Iowa (us-central1), South Carolina (us-east1), Northern Virginia (us-east4), Columbus (us-east5), Dallas (us-south1), Oregon (us-west1), Los Angeles (us-west2), Salt Lake City (us-west3), Las Vegas (us-west4)</p>

--- FULL RAW ENTRY ---
{
  "title": "RESOLVED: Multiple GCP products are experiencing Service issues",
  "title_detail": {
    "type": "text/plain",
    "language": null,
    "base": "https://status.cloud.google.com/feed.atom",
    "value": "RESOLVED: Multiple GCP products are experiencing Service issues"
  },
  "links": [
    {
      "href": "https://status.cloud.google.com/incidents/ow5i3PPK96RduMcb1SsW",
      "rel": "alternate",
      "type": "text/html"
    }
  ],
  "link": "https://status.cloud.google.com/incidents/ow5i3PPK96RduMcb1SsW",
  "id": "tag:status.cloud.google.com,2025:feed:ow5i3PPK96RduMcb1SsW.Z1Rd9fBcpcBK2yggo9W8",
  "guidislink": false,
  "updated": "2025-06-13T23:48:18+00:00",
  "updated_parsed": [
    2025,
    6,
    13,
    23,
    48,
    18,
    4,
    164,
    0
  ],
  "summary": "<p> Incident began at <strong>2025-06-12 10:51</strong> and ended at <strong>2025-06-12 18:18</strong> <span>(all times are <strong>US/Pacific</strong>).</span></p><div class=\"cBIRi14aVDP__status-update-text\"><h1>Incident Report</h1>\n<h2><strong>Summary</strong></h2>\n<p><em>Google Cloud, Google Workspace and Google Security Operations products experienced increased 503 errors in external API requests, impacting customers.</em></p>\n<p><em><strong>We deeply apologize for the impact this outage has had.  Google Cloud customers and their users trust their businesses to Google, and we will do better.  We apologize for the impact this has had not only on our customers\u2019 businesses and their users but also on the trust of our systems.  We are committed to making improvements to help avoid outages like this moving forward.</strong></em></p>\n<h3><strong>What happened?</strong></h3>\n<p>Google and Google Cloud APIs are served through our Google API management and control planes.  Distributed regionally, these management and control planes are responsible for ensuring each API request that comes in is authorized, has the policy and appropriate checks (like quota) to meet their endpoints.  The core binary that is part of this policy check system is known as Service Control.  Service Control is a regional service that has a regional datastore that it reads quota and policy information from. This datastore metadata gets replicated almost instantly globally to manage quota policies for Google Cloud and our customers.</p>\n<p>On May 29, 2025, a new feature was added to Service Control for additional quota policy checks. This code change and binary release went through our region by region rollout, but the code path that failed was never exercised during this rollout due to needing a policy change that would trigger the code.  As a safety precaution, this code change came with a red-button to turn off that particular policy serving path.  The issue with this change was that it did not have appropriate error handling nor was it feature flag protected.  Without the appropriate error handling, the null pointer caused the binary to crash. Feature flags are used to gradually enable the feature region by region per project, starting with internal projects, to enable us to catch issues.  If this had been flag protected, the issue would have been caught in staging.</p>\n<p>On June 12, 2025 at ~10:45am PDT, a policy change was inserted into the regional Spanner tables that Service Control uses for policies.  Given the global nature of quota management, this metadata was replicated globally within seconds.  This policy data contained unintended blank fields.  Service Control, then regionally exercised quota checks on policies in each regional datastore. This pulled in blank fields for this respective policy change and exercised the code path that hit the null pointer causing the binaries to go into a crash loop.  This occurred globally given each regional deployment.</p>\n<p>Within 2 minutes, our Site Reliability Engineering team was triaging the incident.  Within 10 minutes, the root cause was identified and the red-button (to disable the serving path) was being put in place.  The red-button was ready to roll out ~25 minutes from the start of the incident.  Within 40 minutes of the incident, the red-button rollout was completed, and we started seeing recovery across regions, starting with the smaller ones first.</p>\n<p>Within some of our larger regions, such as us-central-1, as Service Control tasks restarted, it created a herd effect on the underlying infrastructure it depends on (i.e. that Spanner table), overloading the infrastructure.  Service Control did not have the appropriate randomized exponential backoff implemented to avoid this.  It took up to ~2h 40 mins to fully resolve in us-central-1 as we throttled task creation to minimize the impact on the underlying infrastructure and routed traffic to multi-regional databases to reduce the load.  At that point, Service Control and API serving was fully recovered across all regions.  Corresponding Google and Google Cloud products started recovering with some taking longer depending upon their architecture.</p>\n<h3><strong>What is our immediate path forward?</strong></h3>\n<p>Immediately upon recovery, we froze all changes to the Service Control stack and manual policy pushes until we can completely remediate the system.</p>\n<h3><strong>How did we communicate?</strong></h3>\n<p>We posted our first incident report to Cloud Service Health about ~1h after the start of the crashes, due to the Cloud Service Health infrastructure being down due to this outage. For some customers, the monitoring infrastructure they had running on Google Cloud was also failing, leaving them without a signal of the incident or an understanding of the impact to their business and/or infrastructure. We will address this going forward.</p>\n<h3><strong>What\u2019s our approach moving forward?</strong></h3>\n<p>Beyond freezing the system as mentioned above, we will prioritize and safely complete the following:</p>\n<ul>\n<li>We will modularize Service Control\u2019s architecture, so the functionality is isolated and fails open.  Thus, if a corresponding check fails, Service Control can still serve API requests.</li>\n<li>We will audit all systems that consume globally replicated data.  Regardless of the business need for near instantaneous consistency of the data globally (i.e. quota management settings are global), data replication needs to be propagated incrementally with sufficient time to validate and detect issues.</li>\n<li>We will enforce all changes to critical binaries to be feature flag protected and disabled by default.</li>\n<li>We will improve our static analysis and testing practices to correctly handle errors and if need be fail open.</li>\n<li>We will audit and ensure our systems employ randomized exponential backoff.</li>\n<li>We will improve our external communications, both automated and human, so our customers get the information they need asap to react to issues, manage their systems and help their customers.</li>\n<li>We'll ensure our monitoring and communication infrastructure remains operational to serve customers even when Google Cloud and our primary monitoring products are down, ensuring business continuity.</li>\n</ul>\n<hr />\n</div><hr /><p>Affected products: API Gateway, Agent Assist, AlloyDB for PostgreSQL, Apigee, Apigee Edge Public Cloud, Apigee Hybrid, Cloud Data Fusion, Cloud Firestore, Cloud Logging, Cloud Memorystore, Cloud Monitoring, Cloud Run, Cloud Security Command Center, Cloud Shell, Cloud Spanner, Cloud Workstations, Contact Center AI Platform, Contact Center Insights, Data Catalog, Database Migration Service, Dataform, Dataplex, Dataproc Metastore, Datastream, Dialogflow CX, Dialogflow ES, Google App Engine, Google BigQuery, Google Cloud Bigtable, Google Cloud Composer, Google Cloud Console, Google Cloud DNS, Google Cloud Dataflow, Google Cloud Dataproc, Google Cloud Pub/Sub, Google Cloud SQL, Google Cloud Storage, Google Compute Engine, Identity Platform, Identity and Access Management, Looker Studio, Managed Service for Apache Kafka, Memorystore for Memcached, Memorystore for Redis, Memorystore for Redis Cluster, Persistent Disk, Personalized Service Health, Pub/Sub Lite, Speech-to-Text, Text-to-Speech, Vertex AI Online Prediction, Vertex AI Search, Vertex Gemini API, Vertex Imagen API, reCAPTCHA Enterprise</p><p>Affected locations: Johannesburg (africa-south1), Multi-region: asia, Taiwan (asia-east1), Hong Kong (asia-east2), Tokyo (asia-northeast1), Osaka (asia-northeast2), Seoul (asia-northeast3), Mumbai (asia-south1), Delhi (asia-south2), Singapore (asia-southeast1), Jakarta (asia-southeast2), Multi-region: asia1, Sydney (australia-southeast1), Melbourne (australia-southeast2), Multi-region: eu, Multi-region: eur3, Multi-region: eur4, Multi-region: eur5, Warsaw (europe-central2), Finland (europe-north1), Stockholm (europe-north2), Madrid (europe-southwest1), Belgium (europe-west1), Berlin (europe-west10), Turin (europe-west12), London (europe-west2), Frankfurt (europe-west3), Netherlands (europe-west4), Zurich (europe-west6), Milan (europe-west8), Paris (europe-west9), Global, Doha (me-central1), Dammam (me-central2), Tel Aviv (me-west1), Multi-region: nam-eur-asia1, Multi-region: nam10, Multi-region: nam11, Multi-region: nam12, Multi-region: nam13, Multi-region: nam3, Multi-region: nam5, Multi-region: nam6, Multi-region: nam7, Multi-region: nam8, Multi-region: nam9, Montr\u00e9al (northamerica-northeast1), Toronto (northamerica-northeast2), Mexico (northamerica-south1), S\u00e3o Paulo (southamerica-east1), Santiago (southamerica-west1), Multi-region: us, Iowa (us-central1), South Carolina (us-east1), Northern Virginia (us-east4), Columbus (us-east5), Dallas (us-south1), Oregon (us-west1), Los Angeles (us-west2), Salt Lake City (us-west3), Las Vegas (us-west4)</p>",
  "summary_detail": {
    "type": "text/html",
    "language": null,
    "base": "https://status.cloud.google.com/feed.atom",
    "value": "<p> Incident began at <strong>2025-06-12 10:51</strong> and ended at <strong>2025-06-12 18:18</strong> <span>(all times are <strong>US/Pacific</strong>).</span></p><div class=\"cBIRi14aVDP__status-update-text\"><h1>Incident Report</h1>\n<h2><strong>Summary</strong></h2>\n<p><em>Google Cloud, Google Workspace and Google Security Operations products experienced increased 503 errors in external API requests, impacting customers.</em></p>\n<p><em><strong>We deeply apologize for the impact this outage has had.  Google Cloud customers and their users trust their businesses to Google, and we will do better.  We apologize for the impact this has had not only on our customers\u2019 businesses and their users but also on the trust of our systems.  We are committed to making improvements to help avoid outages like this moving forward.</strong></em></p>\n<h3><strong>What happened?</strong></h3>\n<p>Google and Google Cloud APIs are served through our Google API management and control planes.  Distributed regionally, these management and control planes are responsible for ensuring each API request that comes in is authorized, has the policy and appropriate checks (like quota) to meet their endpoints.  The core binary that is part of this policy check system is known as Service Control.  Service Control is a regional service that has a regional datastore that it reads quota and policy information from. This datastore metadata gets replicated almost instantly globally to manage quota policies for Google Cloud and our customers.</p>\n<p>On May 29, 2025, a new feature was added to Service Control for additional quota policy checks. This code change and binary release went through our region by region rollout, but the code path that failed was never exercised during this rollout due to needing a policy change that would trigger the code.  As a safety precaution, this code change came with a red-button to turn off that particular policy serving path.  The issue with this change was that it did not have appropriate error handling nor was it feature flag protected.  Without the appropriate error handling, the null pointer caused the binary to crash. Feature flags are used to gradually enable the feature region by region per project, starting with internal projects, to enable us to catch issues.  If this had been flag protected, the issue would have been caught in staging.</p>\n<p>On June 12, 2025 at ~10:45am PDT, a policy change was inserted into the regional Spanner tables that Service Control uses for policies.  Given the global nature of quota management, this metadata was replicated globally within seconds.  This policy data contained unintended blank fields.  Service Control, then regionally exercised quota checks on policies in each regional datastore. This pulled in blank fields for this respective policy change and exercised the code path that hit the null pointer causing the binaries to go into a crash loop.  This occurred globally given each regional deployment.</p>\n<p>Within 2 minutes, our Site Reliability Engineering team was triaging the incident.  Within 10 minutes, the root cause was identified and the red-button (to disable the serving path) was being put in place.  The red-button was ready to roll out ~25 minutes from the start of the incident.  Within 40 minutes of the incident, the red-button rollout was completed, and we started seeing recovery across regions, starting with the smaller ones first.</p>\n<p>Within some of our larger regions, such as us-central-1, as Service Control tasks restarted, it created a herd effect on the underlying infrastructure it depends on (i.e. that Spanner table), overloading the infrastructure.  Service Control did not have the appropriate randomized exponential backoff implemented to avoid this.  It took up to ~2h 40 mins to fully resolve in us-central-1 as we throttled task creation to minimize the impact on the underlying infrastructure and routed traffic to multi-regional databases to reduce the load.  At that point, Service Control and API serving was fully recovered across all regions.  Corresponding Google and Google Cloud products started recovering with some taking longer depending upon their architecture.</p>\n<h3><strong>What is our immediate path forward?</strong></h3>\n<p>Immediately upon recovery, we froze all changes to the Service Control stack and manual policy pushes until we can completely remediate the system.</p>\n<h3><strong>How did we communicate?</strong></h3>\n<p>We posted our first incident report to Cloud Service Health about ~1h after the start of the crashes, due to the Cloud Service Health infrastructure being down due to this outage. For some customers, the monitoring infrastructure they had running on Google Cloud was also failing, leaving them without a signal of the incident or an understanding of the impact to their business and/or infrastructure. We will address this going forward.</p>\n<h3><strong>What\u2019s our approach moving forward?</strong></h3>\n<p>Beyond freezing the system as mentioned above, we will prioritize and safely complete the following:</p>\n<ul>\n<li>We will modularize Service Control\u2019s architecture, so the functionality is isolated and fails open.  Thus, if a corresponding check fails, Service Control can still serve API requests.</li>\n<li>We will audit all systems that consume globally replicated data.  Regardless of the business need for near instantaneous consistency of the data globally (i.e. quota management settings are global), data replication needs to be propagated incrementally with sufficient time to validate and detect issues.</li>\n<li>We will enforce all changes to critical binaries to be feature flag protected and disabled by default.</li>\n<li>We will improve our static analysis and testing practices to correctly handle errors and if need be fail open.</li>\n<li>We will audit and ensure our systems employ randomized exponential backoff.</li>\n<li>We will improve our external communications, both automated and human, so our customers get the information they need asap to react to issues, manage their systems and help their customers.</li>\n<li>We'll ensure our monitoring and communication infrastructure remains operational to serve customers even when Google Cloud and our primary monitoring products are down, ensuring business continuity.</li>\n</ul>\n<hr />\n</div><hr /><p>Affected products: API Gateway, Agent Assist, AlloyDB for PostgreSQL, Apigee, Apigee Edge Public Cloud, Apigee Hybrid, Cloud Data Fusion, Cloud Firestore, Cloud Logging, Cloud Memorystore, Cloud Monitoring, Cloud Run, Cloud Security Command Center, Cloud Shell, Cloud Spanner, Cloud Workstations, Contact Center AI Platform, Contact Center Insights, Data Catalog, Database Migration Service, Dataform, Dataplex, Dataproc Metastore, Datastream, Dialogflow CX, Dialogflow ES, Google App Engine, Google BigQuery, Google Cloud Bigtable, Google Cloud Composer, Google Cloud Console, Google Cloud DNS, Google Cloud Dataflow, Google Cloud Dataproc, Google Cloud Pub/Sub, Google Cloud SQL, Google Cloud Storage, Google Compute Engine, Identity Platform, Identity and Access Management, Looker Studio, Managed Service for Apache Kafka, Memorystore for Memcached, Memorystore for Redis, Memorystore for Redis Cluster, Persistent Disk, Personalized Service Health, Pub/Sub Lite, Speech-to-Text, Text-to-Speech, Vertex AI Online Prediction, Vertex AI Search, Vertex Gemini API, Vertex Imagen API, reCAPTCHA Enterprise</p><p>Affected locations: Johannesburg (africa-south1), Multi-region: asia, Taiwan (asia-east1), Hong Kong (asia-east2), Tokyo (asia-northeast1), Osaka (asia-northeast2), Seoul (asia-northeast3), Mumbai (asia-south1), Delhi (asia-south2), Singapore (asia-southeast1), Jakarta (asia-southeast2), Multi-region: asia1, Sydney (australia-southeast1), Melbourne (australia-southeast2), Multi-region: eu, Multi-region: eur3, Multi-region: eur4, Multi-region: eur5, Warsaw (europe-central2), Finland (europe-north1), Stockholm (europe-north2), Madrid (europe-southwest1), Belgium (europe-west1), Berlin (europe-west10), Turin (europe-west12), London (europe-west2), Frankfurt (europe-west3), Netherlands (europe-west4), Zurich (europe-west6), Milan (europe-west8), Paris (europe-west9), Global, Doha (me-central1), Dammam (me-central2), Tel Aviv (me-west1), Multi-region: nam-eur-asia1, Multi-region: nam10, Multi-region: nam11, Multi-region: nam12, Multi-region: nam13, Multi-region: nam3, Multi-region: nam5, Multi-region: nam6, Multi-region: nam7, Multi-region: nam8, Multi-region: nam9, Montr\u00e9al (northamerica-northeast1), Toronto (northamerica-northeast2), Mexico (northamerica-south1), S\u00e3o Paulo (southamerica-east1), Santiago (southamerica-west1), Multi-region: us, Iowa (us-central1), South Carolina (us-east1), Northern Virginia (us-east4), Columbus (us-east5), Dallas (us-south1), Oregon (us-west1), Los Angeles (us-west2), Salt Lake City (us-west3), Las Vegas (us-west4)</p>"
  }
}